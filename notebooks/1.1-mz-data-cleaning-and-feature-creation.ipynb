{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2c76d114",
   "metadata": {},
   "source": [
    "# Data cleaning and feature creation\n",
    "\n",
    "## 1. Imports and config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "93a99c89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import pandas as pd\n",
    "import re\n",
    "from loguru import logger\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Import your existing config\n",
    "from src.config import DATA_FILES, PROCESSED_DATA_DIR, FIGURES_DIR\n",
    "\n",
    "# Set up plotting style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56c4c70e",
   "metadata": {},
   "source": [
    "## 2. Load raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a0bfcc37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-08-09 21:58:12.840\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m1\u001b[0m - \u001b[1mLoading raw datasets...\u001b[0m\n",
      "Train shape: (10240, 2)\n",
      "Test shape: (2551, 2)\n",
      "Train columns: ['Statement', 'Label']\n",
      "\n",
      "Sample training data:\n",
      "                                           Statement  Label\n",
      "0  Says the Annies List political group supports ...  False\n",
      "1  When did the decline of coal start? It started...   True\n",
      "2  Hillary Clinton agrees with John McCain \"by vo...   True\n",
      "3  Health care reform legislation is likely to ma...  False\n",
      "4  The economic turnaround started at the end of ...   True\n",
      "\n",
      "Class distribution:\n",
      "Label\n",
      "True     5752\n",
      "False    4488\n",
      "Name: count, dtype: int64\n",
      "Class balance ratio: 1.28:1\n"
     ]
    }
   ],
   "source": [
    "logger.info(\"Loading raw datasets...\")\n",
    "\n",
    "train_df = pd.read_csv(DATA_FILES[\"train_raw\"])\n",
    "test_df = pd.read_csv(DATA_FILES[\"test_raw\"])\n",
    "\n",
    "print(f\"Train shape: {train_df.shape}\")\n",
    "print(f\"Test shape: {test_df.shape}\")\n",
    "print(f\"Train columns: {list(train_df.columns)}\")\n",
    "\n",
    "# Display sample data\n",
    "print(\"\\nSample training data:\")\n",
    "print(train_df.head())\n",
    "\n",
    "print(\"\\nClass distribution:\")\n",
    "if 'Label' in train_df.columns:\n",
    "    class_dist = train_df['Label'].value_counts()\n",
    "    print(class_dist)\n",
    "    print(f\"Class balance ratio: {class_dist.max() / class_dist.min():.2f}:1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "847e529a",
   "metadata": {},
   "source": [
    "## 3. Data quality analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "af19cb6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Data Quality Analysis\n",
      "------------------------------\n",
      "Missing values in training data:\n",
      "No missing values found\n",
      "\n",
      "Duplicate statements: 17 (0.2%)\n",
      "\n",
      "Text length statistics:\n",
      "  Mean: 106.9 characters\n",
      "  Median: 99.0 characters\n",
      "  Min: 11\n",
      "  Max: 3145\n",
      "  Texts < 50 chars: 667 (6.5%)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nData Quality Analysis\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# Missing values\n",
    "missing_train = train_df.isnull().sum()\n",
    "print(f\"Missing values in training data:\")\n",
    "for col, missing in missing_train.items():\n",
    "    if missing > 0:\n",
    "        print(f\"  {col}: {missing} ({missing/len(train_df)*100:.1f}%)\")\n",
    "\n",
    "if missing_train.sum() == 0:\n",
    "    print(\"No missing values found\")\n",
    "\n",
    "# Duplicates\n",
    "duplicates = train_df['Statement'].duplicated().sum()\n",
    "print(f\"\\nDuplicate statements: {duplicates} ({duplicates/len(train_df)*100:.1f}%)\")\n",
    "\n",
    "# Text length analysis\n",
    "train_df['original_length'] = train_df['Statement'].astype(str).str.len()\n",
    "print(f\"\\nText length statistics:\")\n",
    "print(f\"  Mean: {train_df['original_length'].mean():.1f} characters\")\n",
    "print(f\"  Median: {train_df['original_length'].median():.1f} characters\")\n",
    "print(f\"  Min: {train_df['original_length'].min()}\")\n",
    "print(f\"  Max: {train_df['original_length'].max()}\")\n",
    "\n",
    "# Very short texts\n",
    "short_threshold = 50\n",
    "short_texts = (train_df['original_length'] < short_threshold).sum()\n",
    "print(f\"  Texts < {short_threshold} chars: {short_texts} ({short_texts/len(train_df)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1676309",
   "metadata": {},
   "source": [
    "## 4. Text cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "65db4b77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enhanced Text Cleaning\n",
      "------------------------------\n",
      "Applying advanced text cleaning...\n",
      "âœ… Text cleaning complete\n"
     ]
    }
   ],
   "source": [
    "print(\"Enhanced Text Cleaning\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "def advanced_text_cleaning(text):\n",
    "    \"\"\"Advanced text cleaning pipeline.\"\"\"\n",
    "    if pd.isna(text) or not isinstance(text, str):\n",
    "        return \"\"\n",
    "    \n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "    \n",
    "    # Remove email addresses\n",
    "    text = re.sub(r'\\S+@\\S+', '', text)\n",
    "    \n",
    "    # Remove HTML tags\n",
    "    text = re.sub(r'<[^<]+?>', '', text)\n",
    "    \n",
    "    # Remove special characters but keep basic punctuation temporarily\n",
    "    text = re.sub(r'[^\\w\\s!?.,;:]', ' ', text)\n",
    "    \n",
    "    # Remove extra whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    return text\n",
    "\n",
    "def create_modeling_text(text):\n",
    "    \"\"\"Final cleaning for model input.\"\"\"\n",
    "    if pd.isna(text) or not isinstance(text, str):\n",
    "        return \"\"\n",
    "    \n",
    "    # Remove punctuation and numbers for TF-IDF\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    \n",
    "    # Remove extra whitespace and very short words\n",
    "    words = text.split()\n",
    "    words = [word for word in words if len(word) >= 3]\n",
    "    \n",
    "    return ' '.join(words)\n",
    "\n",
    "# Apply cleaning\n",
    "print(\"Applying advanced text cleaning...\")\n",
    "train_df['statement_cleaned'] = train_df['Statement'].apply(advanced_text_cleaning)\n",
    "test_df['statement_cleaned'] = test_df['Statement'].apply(advanced_text_cleaning)\n",
    "\n",
    "# Create final modeling text\n",
    "train_df['cleaned_text'] = train_df['statement_cleaned'].apply(create_modeling_text)\n",
    "test_df['cleaned_text'] = test_df['statement_cleaned'].apply(create_modeling_text)\n",
    "\n",
    "print(\"Text cleaning complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea018113",
   "metadata": {},
   "source": [
    "## 5. Feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fc0bafc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Engineering\n",
      "------------------------------\n",
      "Creating text features...\n",
      "Created 12 new features\n"
     ]
    }
   ],
   "source": [
    "print(\"Feature Engineering\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "def create_text_features(df, text_col='Statement'):\n",
    "    \"\"\"Create comprehensive text-based features.\"\"\"\n",
    "    df_enhanced = df.copy()\n",
    "    \n",
    "    # Basic length features\n",
    "    df_enhanced['text_length'] = df[text_col].astype(str).str.len()\n",
    "    df_enhanced['word_count'] = df[text_col].astype(str).str.split().str.len()\n",
    "    df_enhanced['avg_word_length'] = df_enhanced['text_length'] / (df_enhanced['word_count'] + 1)\n",
    "    \n",
    "    # Punctuation features\n",
    "    df_enhanced['exclamation_count'] = df[text_col].astype(str).str.count(r'!')\n",
    "    df_enhanced['question_count'] = df[text_col].astype(str).str.count(r'\\?')\n",
    "    df_enhanced['period_count'] = df[text_col].astype(str).str.count(r'\\.')\n",
    "    \n",
    "    # Capitalization features\n",
    "    df_enhanced['caps_count'] = df[text_col].astype(str).apply(\n",
    "        lambda x: sum(1 for c in str(x) if c.isupper())\n",
    "    )\n",
    "    df_enhanced['caps_ratio'] = df_enhanced['caps_count'] / (df_enhanced['text_length'] + 1)\n",
    "    \n",
    "    # Sentence-level features\n",
    "    df_enhanced['sentence_count'] = df[text_col].astype(str).str.count(r'[.!?]+')\n",
    "    df_enhanced['avg_sentence_length'] = df_enhanced['word_count'] / (df_enhanced['sentence_count'] + 1)\n",
    "    \n",
    "    # Additional features\n",
    "    df_enhanced['digit_count'] = df[text_col].astype(str).str.count(r'\\d')\n",
    "    df_enhanced['digit_ratio'] = df_enhanced['digit_count'] / (df_enhanced['text_length'] + 1)\n",
    "    \n",
    "    return df_enhanced\n",
    "\n",
    "# Create features\n",
    "print(\"Creating text features...\")\n",
    "train_df = create_text_features(train_df)\n",
    "test_df = create_text_features(test_df)\n",
    "\n",
    "feature_columns = [\n",
    "    'text_length', 'word_count', 'avg_word_length',\n",
    "    'exclamation_count', 'question_count', 'period_count',\n",
    "    'caps_count', 'caps_ratio', 'sentence_count', 'avg_sentence_length',\n",
    "    'digit_count', 'digit_ratio'\n",
    "]\n",
    "\n",
    "print(f\"Created {len(feature_columns)} new features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa58b0b2",
   "metadata": {},
   "source": [
    "## 6. Data cleaning and filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e0d68a16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Filtering\n",
      "------------------------------\n",
      "Removed 0 duplicate statements\n",
      "Removed 0 texts shorter than 20 characters\n",
      "Final training samples: 10178\n",
      "Final test samples: 2536\n"
     ]
    }
   ],
   "source": [
    "print(\"Data Filtering\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# Remove duplicates\n",
    "initial_train_size = len(train_df)\n",
    "train_df = train_df.drop_duplicates(subset=['Statement'])\n",
    "print(f\"Removed {initial_train_size - len(train_df)} duplicate statements\")\n",
    "\n",
    "# Filter very short texts\n",
    "min_length = 20\n",
    "initial_size = len(train_df)\n",
    "train_df = train_df[train_df['cleaned_text'].str.len() >= min_length]\n",
    "print(f\"Removed {initial_size - len(train_df)} texts shorter than {min_length} characters\")\n",
    "\n",
    "# Apply same filtering to test set\n",
    "test_df = test_df[test_df['cleaned_text'].str.len() >= min_length]\n",
    "\n",
    "# Remove empty texts\n",
    "train_df = train_df[train_df['cleaned_text'].str.len() > 0]\n",
    "test_df = test_df[test_df['cleaned_text'].str.len() > 0]\n",
    "\n",
    "print(f\"Final training samples: {len(train_df)}\")\n",
    "print(f\"Final test samples: {len(test_df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34ea40b4",
   "metadata": {},
   "source": [
    "## 7. Train/test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1f71cfd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Train/Validation Split\n",
      "------------------------------\n",
      "Training set: 8142 samples\n",
      "Validation set: 2036 samples\n",
      "Train class distribution: {True: 4580, False: 3562}\n",
      "Val class distribution: {True: 1145, False: 891}\n"
     ]
    }
   ],
   "source": [
    "print(\"Creating Train/Validation Split\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# Select columns for final dataset\n",
    "final_columns = ['cleaned_text'] + feature_columns\n",
    "if 'Label' in train_df.columns:\n",
    "    final_columns.append('Label')\n",
    "\n",
    "X = train_df[final_columns]\n",
    "y = train_df['Label'] if 'Label' in train_df.columns else None\n",
    "\n",
    "if y is not None:\n",
    "    # Stratified split\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42, stratify=y\n",
    "    )\n",
    "    \n",
    "    print(f\"Training set: {len(X_train)} samples\")\n",
    "    print(f\"Validation set: {len(X_val)} samples\")\n",
    "    print(f\"Train class distribution: {y_train.value_counts().to_dict()}\")\n",
    "    print(f\"Val class distribution: {y_val.value_counts().to_dict()}\")\n",
    "else:\n",
    "    X_train = X\n",
    "    X_val = None\n",
    "    print(\"No labels found - using all data for training\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6eb59ca",
   "metadata": {},
   "source": [
    "## 8. Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e769a807",
   "metadata": {},
   "source": [
    "## 9. Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0b8ff54a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ENHANCED PREPROCESSING COMPLETE\n",
      "============================================================\n",
      "Final Dataset Summary:\n",
      "Training samples: 8142\n",
      "Validation samples: 2036\n",
      "Test samples: 2536\n",
      "Features per sample: 13\n",
      "Features created:\n",
      "   1. text_length\n",
      "   2. word_count\n",
      "   3. avg_word_length\n",
      "   4. exclamation_count\n",
      "   5. question_count\n",
      "   6. period_count\n",
      "   7. caps_count\n",
      "   8. caps_ratio\n",
      "   9. sentence_count\n",
      "  10. avg_sentence_length\n",
      "  11. digit_count\n",
      "  12. digit_ratio\n",
      "\n",
      "ðŸ“ˆ Key improvements:\n",
      "   âœ… Advanced text cleaning (URLs, HTML, emails removed)\n",
      "   âœ… 12 engineered features\n",
      "   âœ… Duplicate removal\n",
      "   âœ… Quality filtering\n",
      "   âœ… Proper train/validation split\n",
      "\n",
      "ðŸš€ Next steps:\n",
      "   1. Run training: make train\n",
      "   2. The enhanced modeling.py will automatically use these features\n",
      "   3. Expect significant performance improvement!\n",
      "\u001b[32m2025-08-09 22:36:48.271\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m28\u001b[0m - \u001b[1mEnhanced preprocessing notebook execution complete! ðŸŽ¯\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "test_final = test_df[final_columns[:-1]]  # Remove label column if it exists\n",
    "\n",
    "print(\"ENHANCED PREPROCESSING COMPLETE\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"Final Dataset Summary:\")\n",
    "print(f\"Training samples: {len(X_train)}\")\n",
    "print(f\"Validation samples: {len(X_val) if X_val is not None else 0}\")\n",
    "print(f\"Test samples: {len(test_final)}\")\n",
    "print(f\"Features per sample: {len(final_columns)-1}\")  # -1 for label\n",
    "\n",
    "print(\"Features created:\")\n",
    "for i, feature in enumerate(feature_columns, 1):\n",
    "    print(f\"  {i:2d}. {feature}\")\n",
    "\n",
    "logger.success(\"Enhanced preprocessing notebook execution complete.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
